{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a341e5c7-281e-4896-ac1b-0a07e006f581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Udacity Project: Divvy Bikeshare dataset\n",
    "\n",
    "In this project, you'll build a data lake solution for Divvy bikeshare.\n",
    "\n",
    "Divvy is a bike sharing program in Chicago, Illinois USA that allows riders to purchase a pass at a kiosk or use a mobile application to unlock a bike at stations around the city and use the bike for a specified amount of time. The bikes can be returned to the same station or to another station. The City of Chicago makes the anonymized bike trip data publicly available for projects like this where we can analyze the data.\n",
    "\n",
    "Since the data from Divvy are anonymous, we have generated fake rider and account profiles along with fake payment data to go along with the data from Divvy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12dfd2a-66fc-4136-a10c-e504312636da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, DoubleType, DateType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Divvy Bikeshare Dataset\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b964b1b-dc87-459f-9867-3c94e901cdfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Extract Step\n",
    "The notebook should contain Python code to extract information from CSV files stored in Databricks and write it to the Delta file system.\n",
    "\n",
    "## Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b1d376e-cd21-44a5-9a1d-28f50ee1ccc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"payment_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"date\", DateType(), nullable=False),\n",
    "    StructField(\"amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"account_number\", IntegerType(), nullable=False)\n",
    "])\n",
    "payments_df = spark.read.csv(\"/FileStore/divvy/payments.csv\", schema=schema)\n",
    "payments_df.show()\n",
    "payments_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"/delta/payments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5362055c-e73a-4ce6-ae6b-0dd918c67e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Riders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95cb99cf-597b-424c-92fd-7df16e04d653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"rider_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"first\", StringType(), nullable=False),\n",
    "    StructField(\"last\", StringType(), nullable=False),\n",
    "    StructField(\"address\", StringType(), nullable=False),\n",
    "    StructField(\"birthday\", DateType(), nullable=False),\n",
    "    StructField(\"account_start_date\", DateType(), nullable=False),\n",
    "    StructField(\"account_end_date\", DateType(), nullable=True),\n",
    "    StructField(\"is_member\", StringType(), nullable=False)\n",
    "])\n",
    "riders_df = spark.read.format(\"csv\").load(\"/FileStore/divvy/riders.csv\", schema=schema)\n",
    "riders_df.show()\n",
    "riders_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"/delta/riders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad75b7e-c1e3-4245-b4b3-c7ae2a6c2adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c715e14-3e65-44c6-8f06-c7650a8b9d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"station_key\", StringType(), nullable=False),\n",
    "    StructField(\"name\", StringType(), nullable=False),\n",
    "    StructField(\"lat\", DoubleType(), nullable=False),\n",
    "    StructField(\"long\", DoubleType(), nullable=False)\n",
    "])\n",
    "stations_df = spark.read.format(\"csv\").load(\"/FileStore/divvy/stations.csv\", schema=schema)\n",
    "stations_df.show()\n",
    "stations_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"/delta/stations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "967f5a7f-b478-4cc8-a347-8d2494282af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b79fda4-802d-4dcf-b403-edfd9ee0b051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"trip_id\", StringType(), nullable=False),\n",
    "    StructField(\"rideable_type\", StringType(), nullable=False),\n",
    "    StructField(\"start_at\", TimestampType(), nullable=False),\n",
    "    StructField(\"ended_at\", TimestampType(), nullable=False),\n",
    "    StructField(\"start_station_id\", StringType(), nullable=False),\n",
    "    StructField(\"end_station_id\", StringType(), nullable=False),\n",
    "    StructField(\"rider_id\", IntegerType(), nullable=False)\n",
    "])\n",
    "trips_df = spark.read.format(\"csv\").load(\"/FileStore/divvy/trips.csv\", schema=schema)\n",
    "trips_df.show()\n",
    "trips_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"/delta/trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77fae97a-77d6-4116-902b-77e06b5f6d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load step\n",
    "The notebook should contain code that creates tables and loads data from Delta files. The learner should use spark.sql statements to create the tables and then load data from the files that were extracted in the Extract step.\n",
    "\n",
    "**NOTE**: table schema's are already defined during extract step (when writing to Delta lake above)\n",
    "\n",
    "## Payment table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547f81f6-f431-4e33-8da0-4a4b4e8a12fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = spark.sql(\"\"\"\n",
    "    DROP TABLE IF EXISTS staging_payment;\n",
    "\"\"\")\n",
    "_ = spark.sql(\"\"\"\n",
    "    CREATE TABLE staging_payment\n",
    "    USING DELTA LOCATION \"/delta/payments\";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c920af1-4dfd-45cc-b5f5-6ecde2a62051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Rider table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "711532d8-a30a-409e-b981-54813832c15b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = spark.sql(\"\"\"\n",
    "    DROP TABLE IF EXISTS staging_rider;\n",
    "\"\"\")\n",
    "_ = spark.sql(\"\"\"\n",
    "    CREATE TABLE staging_rider\n",
    "    USING DELTA LOCATION \"/delta/riders\";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "740e5fc8-cfc0-4ec8-a57b-a17eaab3357e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Station table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c6c617-546d-4b10-ba06-f86456f48dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = spark.sql(\"\"\"\n",
    "    DROP TABLE IF EXISTS staging_station;\n",
    "\"\"\")\n",
    "_ = spark.sql(\"\"\"\n",
    "    CREATE TABLE staging_station\n",
    "    USING DELTA LOCATION \"/delta/stations\";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514218ff-7e37-4a9e-a0c7-d0f14a5a5d65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trip table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a725398d-d88e-4d81-a02e-b592bdefd685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_ = spark.sql(\"\"\"\n",
    "    DROP TABLE IF EXISTS staging_trip;\n",
    "\"\"\")\n",
    "_ = spark.sql(\"\"\"\n",
    "    CREATE TABLE staging_trip\n",
    "    USING DELTA LOCATION \"/delta/trips\";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121f4f1a-2334-49a8-aa00-5c37f2789c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Transform Step\n",
    "\n",
    "- The transform scripts should at minimum adhere to the following: should write to delta; should use overwrite mode; save as a table in delta.\n",
    "- The dimension Python scripts should match the schema diagram. Dimensions should generate appropriate keys and should not contain facts.\n",
    "- The fact table Python scripts should contain appropriate keys from the dimensions. In addition, the fact table scripts should appropriately generate the correct facts based on the diagrams provided in the first step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "748ae248-c9c7-4237-a528-0825b7f6038f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dimension tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4869d16-eb9b-4c2d-a616-0ef215752b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Date Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667459bc-1f84-450e-9b19-ef21966617e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "date_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT(CAST(date_format(d,'yyyyMMdd') AS INT)) AS date_key,\n",
    "        d AS date_date,\n",
    "        EXTRACT(YEAR FROM d) AS year,\n",
    "        EXTRACT(QUARTER FROM d) AS quarter,\n",
    "        EXTRACT(MONTH FROM d) AS month,\n",
    "        EXTRACT(WEEK FROM d) AS week_of_year,\n",
    "        EXTRACT(DAYOFWEEK FROM d) AS weekday,\n",
    "        CASE \n",
    "            WHEN weekday(d) IN (5, 6) THEN 1\n",
    "            ELSE 0\n",
    "        END AS is_weekend\n",
    "    FROM (\n",
    "        SELECT TRY_CAST(date AS DATE) AS d FROM staging_payment\n",
    "        UNION ALL\n",
    "        SELECT TRY_CAST(account_start_date AS DATE) FROM staging_rider\n",
    "        UNION ALL\n",
    "        SELECT TRY_CAST(account_end_date AS DATE) FROM staging_rider\n",
    "        UNION ALL\n",
    "        SELECT TRY_CAST(birthday AS DATE) FROM staging_rider\n",
    "        UNION ALL\n",
    "        SELECT TRY_CAST(start_at AS DATE) FROM staging_trip\n",
    "        UNION ALL\n",
    "        SELECT TRY_CAST(ended_at AS DATE) FROM staging_trip\n",
    "    ) t\n",
    "    WHERE d IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "date_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd79332e-85de-4eac-a09d-97b262b0af17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Rider Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51058a3b-2d44-431f-bd67-d7b5b453daa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rider_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        ROW_NUMBER() OVER (ORDER BY TRY_CAST(rider_id AS INT)) AS rider_id,\n",
    "        rider_id AS rider_key,\n",
    "        first AS first,\n",
    "        last AS last,\n",
    "        address AS address,\n",
    "        birthday AS birthday_date,\n",
    "        CAST(date_format(TRY_CAST(account_start_date AS DATE),'yyyyMMdd') AS INT) AS account_start_date_key,\n",
    "        CAST(date_format(TRY_CAST(account_end_date AS DATE),'yyyyMMdd') AS INT) AS account_end_date_key,\n",
    "        CASE WHEN LOWER(TRIM(is_member)) IN ('1','true','yes','y') THEN 1 ELSE 0 END AS is_member,\n",
    "        -- age at account start (years); returns NULL if birthday or account_start_date invalid\n",
    "        CASE \n",
    "        WHEN TRY_CAST(birthday AS DATE) IS NOT NULL AND TRY_CAST(account_start_date AS DATE) IS NOT NULL\n",
    "        THEN DATEDIFF(year, TRY_CAST(birthday AS DATE), TRY_CAST(account_start_date AS DATE))\n",
    "            ELSE NULL \n",
    "        END AS rider_age_at_account_start\n",
    "    FROM staging_rider\n",
    "\"\"\")\n",
    "\n",
    "rider_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_rider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c97927-8fdd-47a9-a35c-b8a6758ded68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5043510b-99af-4700-abb3-04162a83f4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT\n",
    "        hour(dts) * 60 + minute(dts) AS time_key,\n",
    "        hour(dts) AS hour,\n",
    "        minute(dts) AS minute,\n",
    "        CASE \n",
    "            WHEN hour(dts) BETWEEN 5 AND 11 THEN 'morning' \n",
    "            WHEN hour(dts) BETWEEN 12 AND 16 THEN 'afternoon' \n",
    "            WHEN hour(dts) BETWEEN 17 AND 20 THEN 'evening' \n",
    "            ELSE 'night'\n",
    "        END AS time_of_day,\n",
    "        CASE \n",
    "            WHEN hour(dts) IN (7,8,16,17) THEN 1 \n",
    "            ELSE 0 \n",
    "        END AS is_rush_hour\n",
    "    FROM (\n",
    "        SELECT TRY_CAST(start_at AS timestamp) AS dts FROM staging_trip\n",
    "        UNION ALL\n",
    "        SELECT TRY_CAST(ended_at AS timestamp) FROM staging_trip\n",
    "    ) t\n",
    "    WHERE dts IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "date_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc7ff9d-e9ca-4e68-8d38-34e81492edd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Station Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b729a8-9585-4db7-89af-790df11889bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "station_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        ROW_NUMBER() OVER (ORDER BY station_key) AS station_id,\n",
    "        station_key AS station_key,\n",
    "        name AS name,\n",
    "        lat AS lat,\n",
    "        long as long\n",
    "    FROM staging_station\n",
    "\"\"\")\n",
    "\n",
    "station_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b9067d-ac56-4f98-a12a-5333824b0867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fact tables\n",
    "\n",
    "### Trip fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39022fb6-59fe-4db4-855c-d6df7ac6bdd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "trip_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        st.trip_id AS trip_id,\n",
    "        dr.rider_key AS rider_key,\n",
    "        dd_start_date.date_key AS start_date_key,\n",
    "        dt_start_time.time_key AS start_time_key,\n",
    "        dd_end_date.date_key AS end_date_key,\n",
    "        dt_end_time.time_key AS end_time_key,\n",
    "        DATEDIFF(minute, TRY_CAST(st.start_at AS TIMESTAMP), TRY_CAST(st.ended_at AS TIMESTAMP)) AS trip_duration_minutes,\n",
    "        ss.station_key AS start_station_key, \n",
    "        ss.station_key AS end_station_key, \n",
    "        DATEDIFF(year,dr.birthday_date,TRY_CAST(st.start_at AS TIMESTAMP)) AS rider_age_at_trip_start\n",
    "    FROM staging_trip AS st\n",
    "    LEFT JOIN dim_date AS dd_start_date\n",
    "        ON CAST(date_format(TRY_CAST(st.start_at AS TIMESTAMP), 'yyyyMMdd') AS INT) = dd_start_date.date_key\n",
    "    LEFT JOIN dim_time AS dt_start_time\n",
    "        ON EXTRACT(hour FROM TRY_CAST(st.start_at AS TIMESTAMP)) * 60 + EXTRACT(minute FROM TRY_CAST(st.start_at AS TIMESTAMP)) = dt_start_time.time_key\n",
    "    LEFT JOIN dim_date AS dd_end_date\n",
    "        ON CAST(date_format(TRY_CAST(st.ended_at AS TIMESTAMP), 'yyyyMMdd') AS INT) = dd_end_date.date_key\n",
    "    LEFT JOIN dim_time AS dt_end_time\n",
    "        ON EXTRACT(hour FROM TRY_CAST(st.ended_at AS TIMESTAMP)) * 60 + EXTRACT(minute FROM TRY_CAST(st.ended_at AS TIMESTAMP)) = dt_end_time.time_key\n",
    "    LEFT JOIN dim_rider AS dr\n",
    "        ON st.rider_id = dr.rider_key\n",
    "    LEFT JOIN dim_station AS ss\n",
    "        ON st.start_station_id = ss.station_key\n",
    "\"\"\")\n",
    "\n",
    "station_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"fact_trip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e3801c-ded7-43ed-8cf9-89ee3ed08859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Payment fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feee397b-7721-4ef8-a5d0-b25e738c421a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "payment_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sp.payment_id AS payment_id,\n",
    "        dd.date_key AS date_key,\n",
    "        sp.amount AS amount,\n",
    "        dr.rider_key AS rider_sk\n",
    "    FROM staging_payment AS sp\n",
    "    LEFT JOIN dim_date AS dd\n",
    "        ON CAST(date_format(TRY_CAST(sp.date AS DATE), 'yyyyMMdd') AS INT) = dd.date_key\n",
    "    LEFT JOIN dim_rider AS dr\n",
    "      ON sp.account_number = dr.rider_key;\n",
    "\"\"\")\n",
    "\n",
    "station_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"fact_payment\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "divvy_bikeshare",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
