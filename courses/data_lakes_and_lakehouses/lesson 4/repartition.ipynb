{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "from src.config import settings\n",
    "from src.spark_lakehouse import get_spark_session\n",
    "\n",
    "spark = get_spark_session(\"Repartition Example\")\n",
    "\n",
    "df = spark.read.json(settings.SPARK_CLUSTER_DATA_DIR + \"sparkify_log_small.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Explore & do some transformations and actions\n",
    "See how Spark works, especially on the executor tab [of the Spark UI]. For example, write is an action, fill it in with your desired path and look at the executor tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_day = spark.udf.register(\n",
    "    \"get_day\", lambda ts: dt.datetime.fromtimestamp(ts / 1000).day\n",
    ")\n",
    "df_day = df.withColumn(\"day\", get_day(df.ts))\n",
    "df_day.select(\"day\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day.write.partitionBy(\"day\").csv(\n",
    "    settings.SPARK_CLUSTER_DATA_DIR + \"repartitioned_by_day\", mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Repartition\n",
    "Now, try doing repartition. Write another path, and take a look at Executor tab. What changed?\n",
    "\n",
    "**Answer**: Repartition changes the number of partitions that the data is divided into. This can affect how the data is distributed across the executors and can lead to changes in performance and resource utilization. When you repartition the data, Spark will shuffle the data across the cluster to create the specified number of partitions, which can lead to increased network I/O and CPU usage during the shuffle operation.\n",
    "\n",
    "In this particular case, it appaers that partitioning by day creates more partitions (one for each day; 3 in total) than partitioning by the number of workers (2), so some workers have to handle multiple partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day.repartition(2).write.csv(\n",
    "    settings.SPARK_CLUSTER_DATA_DIR + \"repartitioned_2_partitions\", mode=\"overwrite\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-engineering-azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
