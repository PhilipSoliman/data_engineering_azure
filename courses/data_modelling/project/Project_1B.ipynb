{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import time\n",
    "from src.database import (\n",
    "    get_cs_cluster,\n",
    "    get_cs_session,\n",
    "    close_cs_session,\n",
    "    shutdown_cs_cluster,\n",
    "    insert_cs_rows,\n",
    "    create_cs_keyspace,\n",
    "    create_cs_table,\n",
    "    drop_cs_table,\n",
    "    drop_cs_keyspace,\n",
    "    set_cs_keyspace,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data files: 30\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.getcwd() + \"/event_data\"\n",
    "data_files = glob.glob(os.path.join(data_dir, \"*\"))\n",
    "print(f\"Number of data files: {len(data_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data: \t#rows = 8056, \tsize = 67.21 kB (size of list in cache)\n",
      "Filtered data: \t#rows = 6820, \tsize = 854.14 kB (size of file on disk)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_data_rows_list = []\n",
    "for f in data_files:\n",
    "    with open(f, \"r\", encoding=\"utf8\", newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)  # skip header\n",
    "        for line in csvreader:\n",
    "            full_data_rows_list.append(line)\n",
    "\n",
    "print(\n",
    "    f\"Full data: \\t#rows = {len(full_data_rows_list)}, \"\n",
    "    f\"\\tsize = {full_data_rows_list.__sizeof__() / 1000:.2f} kB (size of list in cache)\"\n",
    ")\n",
    "\n",
    "# creating a smaller event data csv file\n",
    "csv.register_dialect(\"myDialect\", quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "datafile_path = \"event_datafile_new.csv\"\n",
    "datafile_header = [\n",
    "    \"artistName\",\n",
    "    \"userFirstName\",\n",
    "    \"userGender\",\n",
    "    \"itemInSession\",\n",
    "    \"userLastName\",\n",
    "    \"songLength\",\n",
    "    \"level\",\n",
    "    \"location\",\n",
    "    \"sessionId\",\n",
    "    \"songTitle\",\n",
    "    \"userId\",\n",
    "]\n",
    "datafile_header_map = {name: idx for idx, name in enumerate(datafile_header)}\n",
    "with open(datafile_path, \"w\", encoding=\"utf8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, dialect=\"myDialect\")\n",
    "    writer.writerow(datafile_header)\n",
    "    for row in full_data_rows_list:\n",
    "        if row[0] == \"\":\n",
    "            continue\n",
    "        writer.writerow(\n",
    "            (\n",
    "                row[0],\n",
    "                row[2],\n",
    "                row[3],\n",
    "                row[4],\n",
    "                row[5],\n",
    "                row[6],\n",
    "                row[7],\n",
    "                row[8],\n",
    "                row[12],\n",
    "                row[13],\n",
    "                row[16],\n",
    "            )\n",
    "        )\n",
    "\n",
    "with open(datafile_path, \"r\", encoding=\"utf8\") as f:\n",
    "    num_filtered_rows = sum(1 for line in f) - 1  # subtract 1 for header\n",
    "    print(\n",
    "        f\"Filtered data: \\t#rows = {num_filtered_rows}, \"\n",
    "        f\"\\tsize = {os.path.getsize(datafile_path) / 1000:.2f} kB (size of file on disk)\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "![](images/image_event_datafile_new.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Cassandra Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Cluster, keyspace and a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = get_cs_cluster()\n",
    "session = get_cs_session(cluster)\n",
    "keyspace_name = \"my_keyspace\"\n",
    "drop_cs_keyspace(session, keyspace_name)  # drop any existing keyspace (nice for reruns)\n",
    "create_cs_keyspace(session, keyspace_name)\n",
    "set_cs_keyspace(session, keyspace_name)  # set the keyspace to the one we just created\n",
    "tables = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries\n",
    "Now we need to create tables to run the following queries. Remember, with Apache Cassandra you model the database tables on the queries you want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "**Assignment**: Give the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession = 4\n",
    "\n",
    "**Query**:\n",
    "```sql\n",
    "SELECT artistName, songTitle, songLength \n",
    "FROM music_history_by_session\n",
    "WHERE sessionId = 338 AND itemInSession = 4;\n",
    "```\n",
    "\n",
    "**Why this table fits the query**\n",
    "- Partition on `sessionId` keeps all rows for a session together.\n",
    "- Clustering on `itemInSession` preserves playback order.\n",
    "- Adding `userId` to the clustering key prevents collisions when `sessionId` is not globally unique. Note: I assumed `sessionId` is not globally unique, as it was not specified in the dataset description. Adding `userId` as a clustering key allows multiple users to have the same `sessionId` without overwriting each other's data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS music_history_by_session (sessionId int, itemInSession int, userId int, artistName text, songTitle text, songLength float, PRIMARY KEY (sessionId, itemInSession, userId))\n"
     ]
    }
   ],
   "source": [
    "music_hist_session = \"music_history_by_session\"\n",
    "tables.add(music_hist_session)\n",
    "drop_cs_table(session, music_hist_session)  # drop the table if it exists\n",
    "music_hist_session_cnames = [\n",
    "    \"sessionId\",\n",
    "    \"itemInSession\",\n",
    "    \"userId\",\n",
    "    \"artistName\",\n",
    "    \"songTitle\",\n",
    "    \"songLength\",\n",
    "]\n",
    "music_hist_session_column_types = [\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"text\",\n",
    "    \"text\",\n",
    "    \"float\",\n",
    "]\n",
    "music_hist_session_columns = dict(\n",
    "    zip(music_hist_session_cnames, music_hist_session_column_types)\n",
    ")\n",
    "primary_keys = [\"sessionId\", \"itemInSession\", \"userId\"]\n",
    "create_cs_table(\n",
    "    session,\n",
    "    music_hist_session,\n",
    "    music_hist_session_columns,\n",
    "    primary_keys,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                               \r"
     ]
    }
   ],
   "source": [
    "time.sleep(2)  # wait for table to be created in (local) Cassandra\n",
    "i = 0\n",
    "with open(datafile_path, encoding=\"utf8\") as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader)  # skip header\n",
    "    for line in csvreader:\n",
    "        row = (\n",
    "            int(line[datafile_header_map[\"sessionId\"]]),\n",
    "            int(line[datafile_header_map[\"itemInSession\"]]),\n",
    "            int(line[datafile_header_map[\"userId\"]]),\n",
    "            line[datafile_header_map[\"artistName\"]],\n",
    "            line[datafile_header_map[\"songTitle\"]],\n",
    "            float(line[datafile_header_map[\"songLength\"]]),\n",
    "        )\n",
    "        progress_msg = f\"Progress: Inserting row {i}/{num_filtered_rows} -> {row}\"\n",
    "        print(progress_msg, end=\"\\r\")\n",
    "        insert_cs_rows(session, music_hist_session, music_hist_session_cnames, [row])\n",
    "        print(\" \" * len(progress_msg), end=\"\\r\")  # clear line\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Artist                            Song  Length (s)\n",
      "Faithless Music Matters (Mark Knight Dub)  495.307312\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT artistName, songTitle, songLength \n",
    "FROM music_history_by_session \n",
    "WHERE sessionId = 338 AND itemInSession = 4;\n",
    "\"\"\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "data = [(row.artistname, row.songtitle, row.songlength) for row in rows]\n",
    "df = pd.DataFrame(data, columns=[\"Artist\", \"Song\", \"Length (s)\"])\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "\n",
    "**Assignment**: Give only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "\n",
    "**Query**:\n",
    "```sql\n",
    "SELECT artistName, songTitle, userFirstName, userLastName \n",
    "FROM music_history_by_user\n",
    "WHERE userId = 10 AND sessionId = 182 \n",
    "ORDER BY itemInSession;\n",
    "```\n",
    "\n",
    "**Why this table fits the query**\n",
    "- Composite partition key `(userId, sessionId)` keeps each user-session together and prevents wide partitions across many sessions for one user.\n",
    "- Clustering on `itemInSession` preserves order within the session and keeps rows unique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS music_history_by_user (userId int, sessionId int, itemInSession int, artistName text, songTitle text, userFirstName text, userLastName text, PRIMARY KEY ((userId, sessionId), itemInSession))\n"
     ]
    }
   ],
   "source": [
    "music_hist_user = \"music_history_by_user\"\n",
    "tables.add(music_hist_user)\n",
    "drop_cs_table(session, music_hist_user)  # drop the table if it exists\n",
    "music_hist_user_cnames = [\n",
    "    \"userId\",\n",
    "    \"sessionId\",\n",
    "    \"itemInSession\",\n",
    "    \"artistName\",\n",
    "    \"songTitle\",\n",
    "    \"userFirstName\",\n",
    "    \"userLastName\",\n",
    "]\n",
    "music_hist_user_column_types = [\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"text\",\n",
    "    \"text\",\n",
    "    \"text\",\n",
    "    \"text\",\n",
    "]\n",
    "music_hist_user_columns = dict(\n",
    "    zip(music_hist_user_cnames, music_hist_user_column_types)\n",
    ")\n",
    "primary_keys = [\"(userId, sessionId)\", \"itemInSession\"]\n",
    "create_cs_table(\n",
    "    session,\n",
    "    music_hist_user,\n",
    "    music_hist_user_columns,\n",
    "    primary_keys,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                        \r"
     ]
    }
   ],
   "source": [
    "time.sleep(2)  # wait for table to be created in (local) Cassandra\n",
    "i = 0\n",
    "with open(datafile_path, encoding=\"utf8\") as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader)  # skip header\n",
    "    for line in csvreader:\n",
    "        row = (\n",
    "            int(line[datafile_header_map[\"userId\"]]),\n",
    "            int(line[datafile_header_map[\"sessionId\"]]),\n",
    "            int(line[datafile_header_map[\"itemInSession\"]]),\n",
    "            line[datafile_header_map[\"artistName\"]],\n",
    "            line[datafile_header_map[\"songTitle\"]],\n",
    "            line[datafile_header_map[\"userFirstName\"]],\n",
    "            line[datafile_header_map[\"userLastName\"]],\n",
    "        )\n",
    "        progress_msg = f\"Progress: Inserting row {i}/{num_filtered_rows} -> {row}\"\n",
    "        print(progress_msg, end=\"\\r\")\n",
    "        insert_cs_rows(session, music_hist_user, music_hist_user_cnames, [row])\n",
    "        print(\" \" * len(progress_msg), end=\"\\r\")  # clear line\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Artist                                                 Song First Name Last Name\n",
      " Down To The Bone                                   Keep On Keepin' On     Sylvie      Cruz\n",
      "     Three Drives                                          Greece 2000     Sylvie      Cruz\n",
      "Sebastien Tellier                                            Kilometer     Sylvie      Cruz\n",
      "    Lonnie Gordon Catch You Baby (Steve Pitron & Max Sanna Radio Edit)     Sylvie      Cruz\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT artistName, songTitle, userFirstName, userLastName \n",
    "FROM music_history_by_user\n",
    "WHERE userId = 10 AND sessionId = 182 \n",
    "ORDER BY itemInSession;\n",
    "\"\"\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "data = [\n",
    "    (row.artistname, row.songtitle, row.userfirstname, row.userlastname) for row in rows\n",
    "]\n",
    "df = pd.DataFrame(data, columns=[\"Artist\", \"Song\", \"First Name\", \"Last Name\"])\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "\n",
    "**Assignment**: Give every user name (first and last) in the music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "**Query**:\n",
    "```sql\n",
    "SELECT userFirstName, userLastName \n",
    "FROM music_history_by_song\n",
    "WHERE songTitle = 'All Hands Against His Own';\n",
    "```\n",
    "\n",
    "**Why this table fits the query**\n",
    "- Partition on `songTitle` aligns with the `WHERE songTitle = ...` filter. However, since multiple users can listen to the same song, a simple partition key on `songTitle` would lead to collisions.\n",
    "- Clustering on `userId`, `sessionId`, and `itemInSession` keeps multiple listens unique across users and sessions while allowing repeated plays of the same song.\n",
    "- A downside is that this table could become very wide for popular songs, but it meets the query requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS music_history_by_song (songTitle text, userId int, sessionId int, itemInSession int, userFirstName text, userLastName text, PRIMARY KEY (songTitle, userId, sessionId, itemInSession))\n"
     ]
    }
   ],
   "source": [
    "music_hist_song = \"music_history_by_song\"\n",
    "tables.add(music_hist_song)\n",
    "drop_cs_table(session, music_hist_song)  # drop the table if it exists\n",
    "music_hist_song_cnames = [\n",
    "    \"songTitle\",\n",
    "    \"userId\",\n",
    "    \"sessionId\",\n",
    "    \"itemInSession\",\n",
    "    \"userFirstName\",\n",
    "    \"userLastName\",\n",
    "]\n",
    "music_hist_song_column_types = [\n",
    "    \"text\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "    \"text\",\n",
    "    \"text\",\n",
    "]\n",
    "music_hist_song_columns = dict(\n",
    "    zip(music_hist_song_cnames, music_hist_song_column_types)\n",
    ")\n",
    "primary_keys = [\"songTitle\", \"userId\", \"sessionId\", \"itemInSession\"]\n",
    "create_cs_table(\n",
    "    session,\n",
    "    music_hist_song,\n",
    "    music_hist_song_columns,\n",
    "    primary_keys,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "time.sleep(2)  # wait for tables to be created in (local) Cassandra\n",
    "i = 0\n",
    "with open(datafile_path, encoding=\"utf8\") as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader)  # skip header\n",
    "    for line in csvreader:\n",
    "        row = (\n",
    "            line[datafile_header_map[\"songTitle\"]],\n",
    "            int(line[datafile_header_map[\"userId\"]]),\n",
    "            int(line[datafile_header_map[\"sessionId\"]]),\n",
    "            int(line[datafile_header_map[\"itemInSession\"]]),\n",
    "            line[datafile_header_map[\"userFirstName\"]],\n",
    "            line[datafile_header_map[\"userLastName\"]],\n",
    "        )\n",
    "        progress_msg = f\"Progress: Inserting row {i}/{num_filtered_rows} -> {row}\"\n",
    "        print(progress_msg, end=\"\\r\")\n",
    "        insert_cs_rows(session, music_hist_song, music_hist_song_cnames, [row])\n",
    "        print(\" \" * len(progress_msg), end=\"\\r\")  # clear line\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Name Last Name\n",
      "Jacqueline     Lynch\n",
      "     Tegan    Levine\n",
      "      Sara   Johnson\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT userFirstName, userLastName \n",
    "FROM music_history_by_song\n",
    "WHERE songTitle = 'All Hands Against His Own';\n",
    "\"\"\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "data = [(row.userfirstname, row.userlastname) for row in rows]\n",
    "df = pd.DataFrame(data, columns=[\"First Name\", \"Last Name\"])\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    drop_cs_table(session, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_cs_session(session)\n",
    "shutdown_cs_cluster(cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-engineering-azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
